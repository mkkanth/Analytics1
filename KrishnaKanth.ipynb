{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KrishnaKanth.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXkCSVqfrnUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk as ntk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from google.colab import files\n",
        "import io\n",
        "import nltk.corpus\n",
        "import os\n",
        "from nltk.tokenize import  word_tokenize\n",
        "import csv\n",
        "import spacy\n",
        "import nltk as ntk\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ayKhKApr3j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# this will give a link to your google drive, please click on the link and follow steps, which will give the verification code\n",
        "# once verification code validated then , you should get the sharable link of the file and copy it in the below section"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N18rFzY9sEud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from sklearn import preprocessing, model_selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM2qjM-jsHX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1U2_iClctC8So0Y1J-GHAyXj6AXlEscJg' \n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('drugsComTrain_raw.csv')  \n",
        "fMyfile = pd.read_csv('drugsComTrain_raw.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M64WqcpWJaJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fMyfile = fMyfile[pd.notnull(fMyfile['condition'])]\n",
        "fMyfile.condition =fMyfile.condition.str.replace(r\"[^a-z'A-Z]+\", \" \")\n",
        "fMyfile.condition=fMyfile.condition.str.replace('<[^<]+?>', '')\n",
        "# replace those values in condition columns which has aggreagetd count less than 10\n",
        "s=fMyfile['condition'].value_counts()\n",
        "fMyfile['condition1'] = np.where(fMyfile['condition'].isin(s.index[s >=10]), fMyfile['condition'], 'Other')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzuQNarKsNE9",
        "colab_type": "code",
        "outputId": "7d861ea1-bcbe-4de5-99b9-ac6f8356a36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "fMyfile.review = fMyfile.review.str.replace('&#039;', \"'\")\n",
        "fMyfile.review = fMyfile.review.str.replace('&\\w+;',' ')\n",
        "fMyfile.review = fMyfile.review.str.replace(r\"[^a-z'A-Z]+\", \" \")\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def Remove_UnnecessaryWords(ReviewLine):\n",
        "  finalreview=\" \"\n",
        "  review = ReviewLine['review']\n",
        "  tokens = nltk.word_tokenize(review)\n",
        "  token_words = [w for w in tokens if w.isalpha()]\n",
        "  meaningful_words = [w for w in token_words if not w in stop_words]\n",
        "  finalreview=finalreview.join(meaningful_words)\n",
        "  return(finalreview)\n",
        "fMyfile['review'] = fMyfile.apply(Remove_UnnecessaryWords, axis=1)\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmv1E6s8cwu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "link = \"https://drive.google.com/open?id=1BpWHnVTX8EBU8-p34KhZtdw1wKT5WeBN\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('glove.6B.100d.txt')  \n",
        "glove_file  = open(\"glove.6B.100d.txt\",'r', encoding='utf-8')\n",
        "embeddings_dictionary = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11rKEHNf6Pdk",
        "colab_type": "code",
        "outputId": "fe538858-2bce-4ee6-b418-666d21de93fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "MAX_NB_WORDS = 10000\n",
        "MAX_SEQUENCE_LENGTH = max(fMyfile['review'].apply(len))\n",
        "MAX_SEQUENCE_LENGTH=100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(fMyfile['review'].values)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "word_index = tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(fMyfile['review'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(X.shape)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(160398, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygy1nIa5j_vd",
        "colab_type": "code",
        "outputId": "5d158e20-cbfc-482a-ddf5-161d2874fa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "print(embedding_matrix.shape)\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    try:\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45817, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQSn4zcyw5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "y=fMyfile['condition1']\n",
        "y=pd.get_dummies(y)\n",
        "from keras import utils as np_utils\n",
        "\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,y,test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w03wk1jBy3EX",
        "colab_type": "code",
        "outputId": "d8a32d1e-c694-4ae4-8f45-5247ea5df175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "#model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=100))\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100 , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "#model.add(LSTM(128))\n",
        "model.add(Dense(128, activation='softmax'))\n",
        "model.add(Dense(414, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs =5, batch_size =50)\n",
        "\n",
        "scores = model.evaluate(test_x, test_y)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "128318/128318 [==============================] - 64s 503us/step - loss: 4.7587 - acc: 0.1802\n",
            "Epoch 2/5\n",
            " 28200/128318 [=====>........................] - ETA: 52s - loss: 4.3279 - acc: 0.1756"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdAITek0h-hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSTGKfEilUJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1Spi_frv5FH87kIe_gvm_1GQOxXnMIHrM' \n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('drugsComTest_raw.csv')  \n",
        "Test_new = pd.read_csv('drugsComTest_raw.csv')\n",
        "Xnew = tokenizer.texts_to_sequences(Test_new['review'].values)\n",
        "Xnew = pad_sequences(Xnew, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ynew = model.predict_classes(Xnew)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoev2wZi2jCW",
        "colab_type": "code",
        "outputId": "6acade49-3cec-4ff4-e834-604d72676c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ct=0\n",
        "for i in range(len(ynew)):\n",
        "  if list(y.columns.values)[ynew[i]]==Test_new['condition'][i]:\n",
        "    ct=ct+1\n",
        "print(ct)\n",
        "print((ct/len(ynew))*100)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9862\n",
            "18.34244689952758\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}